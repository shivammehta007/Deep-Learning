{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OSp18hvBgSg",
        "colab_type": "text"
      },
      "source": [
        "* Read Language model tutorial ---> https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
        "* Find one english corpus with poetries in the internet (e.g from here) --> https://www.poetryfoundation.org/poems\n",
        "* You can use whatever corpus you want (e.g. your favorite book)\n",
        "* Encapsulate LSTM building like MLP from the first task\n",
        "* Train LSTM as language model on your corpus like in the tutorial\n",
        "* Also, you need to compare 1-layer and 2-layer LSTMs\n",
        "* Compare texts, generated by your models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3rdTupmBgSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import adam, adagrad, adadelta, rmsprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import L1L2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ParameterGrid, train_test_split\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L38QIwIECiM1",
        "colab_type": "code",
        "outputId": "e1a13659-f81a-45ac-f625-66ad46c7e52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KXJ2f9jBgSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read Data\n",
        "folder_name = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "filename = os.path.join(folder_name, 'task3_corpus.csv')\n",
        "# filename = 'task3_corpus.csv'\n",
        "file_type = 'csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fW7IwapBgS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(filename, file_type):\n",
        "    if file_type == 'csv':\n",
        "        data = pd.read_csv(filename)\n",
        "        data = data['text']\n",
        "        \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwGWpRi_BgTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = read_data(filename, file_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDLP6YnyBgUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = '\\n'.join([row for row in df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REwgMEzJBgUU",
        "colab_type": "code",
        "outputId": "9540e057-9234-4efe-9034-ae937acfcad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "layers = [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)]\n",
        "count = [x for x,_ in layers].count('LSTM')\n",
        "print(count)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Z8RQoVBgUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelFormer:\n",
        "    def __init__(self):\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.best_model = Sequential()\n",
        "        self.best_accuracy = 0\n",
        "        self.best_parameters = {}\n",
        "        \n",
        "    def fit_data(self, text):\n",
        "        self.original_corpus = text\n",
        "        self.corpus = self.original_corpus.lower().split('\\n')\n",
        "        self.tokenizer.fit_on_texts(self.corpus)\n",
        "        self.word_count = len(self.tokenizer.word_index) + 1\n",
        "        input_sequences = []\n",
        "        for line in self.corpus:\n",
        "            tokens = self.tokenizer.texts_to_sequences([line])[0]\n",
        "            for i in range(1, len(tokens)):\n",
        "                n_grams_sequence = tokens[:i+1]\n",
        "                input_sequences.append(n_grams_sequence)\n",
        "        \n",
        "        input_sequences = self.pad_input_sequences(input_sequences)\n",
        "        \n",
        "        x_data, y_data = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "        y_data = np_utils.to_categorical(y_data, num_classes=self.word_count)\n",
        "        \n",
        "        return x_data, y_data\n",
        "              \n",
        "    def pad_input_sequences(self,input_sequences):\n",
        "        max_sequence_length = max([len(sentence) for sentence in input_sequences])\n",
        "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n",
        "        return input_sequences\n",
        "    \n",
        "    def fit(self, x_data, y_data , layers= [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)], activation='tanh', optimizer='adam', lr=0.01, epochs=20):\n",
        "        self.model = Sequential()\n",
        "        \n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        x_train, x_val, y_train, y_val = train_test_split(self.x_data, self.y_data)\n",
        "        \n",
        "        \n",
        "        self.model.add(Embedding(self.word_count, 10, input_length=len(x_data[0]) ))\n",
        "        count_lstm_retn_flag = [x for x,_ in layers].count('LSTM') - 1\n",
        "\n",
        "        for layer,value in layers:\n",
        "            if layer == 'LSTM':\n",
        "                if count_lstm_retn_flag:\n",
        "                    count_lstm_retn_flag -= 1\n",
        "                    return_sequences = True \n",
        "                else:\n",
        "                    return_sequences = False\n",
        "                self.model.add(LSTM(value, activation=activation, return_sequences=return_sequences))\n",
        "            if layer == 'Dropout':\n",
        "                self.model.add(Dropout(value))\n",
        "        \n",
        "        self.model.add(Dense(self.word_count, activation='softmax'))\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = adam(lr=lr)\n",
        "        elif optimizer == 'adadelta':\n",
        "            optimizer = adadelta(lr=lr)\n",
        "        elif optimizer == 'rmsprop':\n",
        "            optimizer = rmsprop(lr=lr)\n",
        "            \n",
        "            \n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.model.summary()\n",
        "        \n",
        "        fit_summary = self.model.fit(x_train, y_train, epochs=epochs, verbose=1, validation_data=(x_val, y_val), batch_size=20)\n",
        "        if fit_summary.history['acc'][-1] > self.best_accuracy:\n",
        "            self.best_model = self.model\n",
        "            self.best_accuracy = fit_summary.history['acc'][-1]\n",
        "            self.best_parameters = (layers, activation, optimizer, lr, epochs)\n",
        "        \n",
        "        return fit_summary\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7C_bMBBgUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = ModelFormer()\n",
        "X, Y= m.fit_data(text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxxUqsczBgU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train , x_test, y_train, y_test = train_test_split(X,Y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEvPZQedBgVA",
        "colab_type": "code",
        "outputId": "e1f1e726-08a0-4b7d-fcc8-432955c1353e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(x_train), len(y_train), len(x_test), len(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8847 8847 3792 3792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMKPolLxBgWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Grid Search with Parameter Grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOAkMxdABgWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters = { 'layers': [ [( 'LSTM', 200), ('Dropout', 0.2)], [( 'LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2) ]], \n",
        "                     'activation': ['tanh'],\n",
        "                     'optimizer' : [ ('adam', 0.01 ), ('adam', 0.001 ) , ('adadelta', 1 ), ('rmsprop', 0.1 )],\n",
        "                     'epochs' : [50]\n",
        "                   }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unOtt-QJBgWn",
        "colab_type": "code",
        "outputId": "ef06ce53-9d57-4a3b-98bc-efa4dab3c5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "combinations = list(ParameterGrid(hyperparameters))\n",
        "combinations"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
              "  'optimizer': ('adam', 0.01)},\n",
              " {'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
              "  'optimizer': ('adam', 0.001)},\n",
              " {'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
              "  'optimizer': ('adadelta', 1)},\n",
              " {'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2)],\n",
              "  'optimizer': ('rmsprop', 0.1)},\n",
              " {'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
              "  'optimizer': ('adam', 0.01)},\n",
              " {'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
              "  'optimizer': ('adam', 0.001)},\n",
              " {'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
              "  'optimizer': ('adadelta', 1)},\n",
              " {'activation': 'tanh',\n",
              "  'epochs': 50,\n",
              "  'layers': [('LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2)],\n",
              "  'optimizer': ('rmsprop', 0.1)}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmkHhKE0WiQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit_summary_array = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EApzDLTlXP5s",
        "colab_type": "code",
        "outputId": "b8a78396-6fb5-407e-91ea-700bbf976a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4760
        }
      },
      "source": [
        "for combination in combinations:\n",
        "    print('Current Combination : {}'.format(combination))\n",
        "    m.fit(x_train, y_train, layers=combination['layers'], activation=combination['activation'], optimizer=combination['optimizer'][0], lr=combination['optimizer'][1], epochs=combination['epochs'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adam', 0.01)}\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_31 (Embedding)     (None, 54, 10)            21850     \n",
            "_________________________________________________________________\n",
            "lstm_44 (LSTM)               (None, 200)               168800    \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 2185)              439185    \n",
            "=================================================================\n",
            "Total params: 629,835\n",
            "Trainable params: 629,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6635 samples, validate on 2212 samples\n",
            "Epoch 1/50\n",
            "6635/6635 [==============================] - 41s 6ms/step - loss: 6.5067 - acc: 0.0524 - val_loss: 6.3879 - val_acc: 0.0633\n",
            "Epoch 2/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.9444 - acc: 0.0735 - val_loss: 6.3883 - val_acc: 0.0999\n",
            "Epoch 3/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.4277 - acc: 0.1177 - val_loss: 6.3533 - val_acc: 0.1171\n",
            "Epoch 4/50\n",
            "6635/6635 [==============================] - 32s 5ms/step - loss: 4.7358 - acc: 0.1604 - val_loss: 6.4929 - val_acc: 0.1307\n",
            "Epoch 5/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 4.0957 - acc: 0.2075 - val_loss: 6.6824 - val_acc: 0.1329\n",
            "Epoch 6/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 3.4244 - acc: 0.2695 - val_loss: 7.0416 - val_acc: 0.1379\n",
            "Epoch 7/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 2.8250 - acc: 0.3595 - val_loss: 7.3537 - val_acc: 0.1388\n",
            "Epoch 8/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 2.3045 - acc: 0.4482 - val_loss: 7.6044 - val_acc: 0.1420\n",
            "Epoch 9/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.9381 - acc: 0.5260 - val_loss: 7.8872 - val_acc: 0.1361\n",
            "Epoch 10/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.6316 - acc: 0.5797 - val_loss: 8.1105 - val_acc: 0.1388\n",
            "Epoch 11/50\n",
            "6635/6635 [==============================] - 32s 5ms/step - loss: 1.4227 - acc: 0.6291 - val_loss: 8.2989 - val_acc: 0.1410\n",
            "Epoch 12/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.2482 - acc: 0.6722 - val_loss: 8.4760 - val_acc: 0.1379\n",
            "Epoch 13/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.1541 - acc: 0.6910 - val_loss: 8.6765 - val_acc: 0.1352\n",
            "Epoch 14/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.1469 - acc: 0.6885 - val_loss: 8.7102 - val_acc: 0.1433\n",
            "Epoch 15/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.1307 - acc: 0.6942 - val_loss: 8.8348 - val_acc: 0.1392\n",
            "Epoch 16/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.1968 - acc: 0.6742 - val_loss: 8.9721 - val_acc: 0.1329\n",
            "Epoch 17/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.2116 - acc: 0.6726 - val_loss: 9.0692 - val_acc: 0.1338\n",
            "Epoch 18/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.2482 - acc: 0.6668 - val_loss: 9.0128 - val_acc: 0.1451\n",
            "Epoch 19/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.3015 - acc: 0.6550 - val_loss: 9.1664 - val_acc: 0.1397\n",
            "Epoch 20/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.3580 - acc: 0.6338 - val_loss: 9.1586 - val_acc: 0.1311\n",
            "Epoch 21/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.7017 - acc: 0.5656 - val_loss: 9.1619 - val_acc: 0.1320\n",
            "Epoch 22/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.5261 - acc: 0.6000 - val_loss: 9.1930 - val_acc: 0.1383\n",
            "Epoch 23/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.5280 - acc: 0.5997 - val_loss: 9.1469 - val_acc: 0.1361\n",
            "Epoch 24/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.5483 - acc: 0.5914 - val_loss: 9.2096 - val_acc: 0.1338\n",
            "Epoch 25/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 1.7684 - acc: 0.5521 - val_loss: 9.2015 - val_acc: 0.1279\n",
            "Epoch 26/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 2.0853 - acc: 0.4971 - val_loss: 9.4092 - val_acc: 0.1121\n",
            "Epoch 27/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 8.1031 - acc: 0.1369 - val_loss: 11.2813 - val_acc: 0.0457\n",
            "Epoch 28/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 10.0357 - acc: 0.0511 - val_loss: 9.8078 - val_acc: 0.0746\n",
            "Epoch 29/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 8.9989 - acc: 0.0745 - val_loss: 9.4686 - val_acc: 0.0782\n",
            "Epoch 30/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 8.4240 - acc: 0.0773 - val_loss: 9.1627 - val_acc: 0.0805\n",
            "Epoch 31/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.7245 - acc: 0.1023 - val_loss: 8.9593 - val_acc: 0.0773\n",
            "Epoch 32/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.3263 - acc: 0.1055 - val_loss: 8.9212 - val_acc: 0.0787\n",
            "Epoch 33/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.0421 - acc: 0.1132 - val_loss: 8.7204 - val_acc: 0.0882\n",
            "Epoch 34/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.5570 - acc: 0.1292 - val_loss: 8.6815 - val_acc: 0.0954\n",
            "Epoch 35/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.3895 - acc: 0.1367 - val_loss: 8.6279 - val_acc: 0.0949\n",
            "Epoch 36/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.1421 - acc: 0.1436 - val_loss: 8.5995 - val_acc: 0.0877\n",
            "Epoch 37/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.0733 - acc: 0.1480 - val_loss: 8.5497 - val_acc: 0.0949\n",
            "Epoch 38/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.5671 - acc: 0.1258 - val_loss: 8.5740 - val_acc: 0.0940\n",
            "Epoch 39/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.5770 - acc: 0.1209 - val_loss: 8.5366 - val_acc: 0.0814\n",
            "Epoch 40/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.0499 - acc: 0.1399 - val_loss: 8.4501 - val_acc: 0.1013\n",
            "Epoch 41/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.6906 - acc: 0.1753 - val_loss: 8.4779 - val_acc: 0.1013\n",
            "Epoch 42/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.4240 - acc: 0.1763 - val_loss: 8.4460 - val_acc: 0.1053\n",
            "Epoch 43/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.1981 - acc: 0.1986 - val_loss: 8.4817 - val_acc: 0.0940\n",
            "Epoch 44/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.2488 - acc: 0.1913 - val_loss: 8.4478 - val_acc: 0.0995\n",
            "Epoch 45/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.2485 - acc: 0.1907 - val_loss: 8.5273 - val_acc: 0.0936\n",
            "Epoch 46/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.3498 - acc: 0.1869 - val_loss: 8.5471 - val_acc: 0.0927\n",
            "Epoch 47/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.4987 - acc: 0.1671 - val_loss: 8.7101 - val_acc: 0.0868\n",
            "Epoch 48/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.9335 - acc: 0.1402 - val_loss: 8.5844 - val_acc: 0.0976\n",
            "Epoch 49/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.8921 - acc: 0.1023 - val_loss: 8.6005 - val_acc: 0.0999\n",
            "Epoch 50/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.8644 - acc: 0.0980 - val_loss: 8.5749 - val_acc: 0.0922\n",
            "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adam', 0.001)}\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_32 (Embedding)     (None, 54, 10)            21850     \n",
            "_________________________________________________________________\n",
            "lstm_45 (LSTM)               (None, 200)               168800    \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 2185)              439185    \n",
            "=================================================================\n",
            "Total params: 629,835\n",
            "Trainable params: 629,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6635 samples, validate on 2212 samples\n",
            "Epoch 1/50\n",
            "6635/6635 [==============================] - 38s 6ms/step - loss: 6.5373 - acc: 0.0567 - val_loss: 6.4296 - val_acc: 0.0488\n",
            "Epoch 2/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.1440 - acc: 0.0577 - val_loss: 6.4876 - val_acc: 0.0488\n",
            "Epoch 3/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.0941 - acc: 0.0576 - val_loss: 6.5381 - val_acc: 0.0488\n",
            "Epoch 4/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.9404 - acc: 0.0579 - val_loss: 6.5729 - val_acc: 0.0484\n",
            "Epoch 5/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.7939 - acc: 0.0644 - val_loss: 6.5707 - val_acc: 0.0588\n",
            "Epoch 6/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.6068 - acc: 0.0719 - val_loss: 6.6170 - val_acc: 0.0628\n",
            "Epoch 7/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.4076 - acc: 0.0833 - val_loss: 6.6274 - val_acc: 0.0687\n",
            "Epoch 8/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.1758 - acc: 0.0965 - val_loss: 6.7071 - val_acc: 0.0755\n",
            "Epoch 9/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.9284 - acc: 0.1124 - val_loss: 6.7270 - val_acc: 0.0732\n",
            "Epoch 10/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.6511 - acc: 0.1289 - val_loss: 6.7990 - val_acc: 0.0805\n",
            "Epoch 11/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.3716 - acc: 0.1503 - val_loss: 6.8559 - val_acc: 0.0823\n",
            "Epoch 12/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.1016 - acc: 0.1794 - val_loss: 6.9150 - val_acc: 0.0814\n",
            "Epoch 13/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 3.8187 - acc: 0.2131 - val_loss: 7.0056 - val_acc: 0.0863\n",
            "Epoch 14/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 3.5488 - acc: 0.2698 - val_loss: 7.0673 - val_acc: 0.0904\n",
            "Epoch 15/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 3.3018 - acc: 0.3097 - val_loss: 7.1307 - val_acc: 0.0972\n",
            "Epoch 16/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 3.0411 - acc: 0.3628 - val_loss: 7.1888 - val_acc: 0.0904\n",
            "Epoch 17/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 2.8167 - acc: 0.4036 - val_loss: 7.2341 - val_acc: 0.0918\n",
            "Epoch 18/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 2.6089 - acc: 0.4417 - val_loss: 7.3030 - val_acc: 0.0936\n",
            "Epoch 19/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 2.4030 - acc: 0.4910 - val_loss: 7.3347 - val_acc: 0.0995\n",
            "Epoch 20/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 2.2371 - acc: 0.5170 - val_loss: 7.4153 - val_acc: 0.0976\n",
            "Epoch 21/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 2.2308 - acc: 0.5185 - val_loss: 7.4578 - val_acc: 0.0995\n",
            "Epoch 22/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 1.9155 - acc: 0.5840 - val_loss: 7.5032 - val_acc: 0.1058\n",
            "Epoch 23/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.7588 - acc: 0.6223 - val_loss: 7.5687 - val_acc: 0.1071\n",
            "Epoch 24/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.6309 - acc: 0.6497 - val_loss: 7.6280 - val_acc: 0.1017\n",
            "Epoch 25/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.5466 - acc: 0.6639 - val_loss: 7.6672 - val_acc: 0.1035\n",
            "Epoch 26/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.4148 - acc: 0.6936 - val_loss: 7.7444 - val_acc: 0.1044\n",
            "Epoch 27/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.3149 - acc: 0.7114 - val_loss: 7.7508 - val_acc: 0.1076\n",
            "Epoch 28/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.2159 - acc: 0.7469 - val_loss: 7.8163 - val_acc: 0.1035\n",
            "Epoch 29/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.1153 - acc: 0.7632 - val_loss: 7.8827 - val_acc: 0.1053\n",
            "Epoch 30/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 1.0414 - acc: 0.7864 - val_loss: 7.9410 - val_acc: 0.1085\n",
            "Epoch 31/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.9572 - acc: 0.8038 - val_loss: 7.9873 - val_acc: 0.1044\n",
            "Epoch 32/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.8936 - acc: 0.8137 - val_loss: 8.0408 - val_acc: 0.1144\n",
            "Epoch 33/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 0.8359 - acc: 0.8298 - val_loss: 8.0893 - val_acc: 0.1071\n",
            "Epoch 34/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.7747 - acc: 0.8433 - val_loss: 8.1413 - val_acc: 0.1071\n",
            "Epoch 35/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.7107 - acc: 0.8568 - val_loss: 8.1920 - val_acc: 0.1139\n",
            "Epoch 36/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.6647 - acc: 0.8675 - val_loss: 8.2808 - val_acc: 0.1067\n",
            "Epoch 37/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.6336 - acc: 0.8731 - val_loss: 8.3129 - val_acc: 0.1117\n",
            "Epoch 38/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.5903 - acc: 0.8827 - val_loss: 8.3810 - val_acc: 0.1180\n",
            "Epoch 39/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.5561 - acc: 0.8918 - val_loss: 8.4182 - val_acc: 0.1094\n",
            "Epoch 40/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.5272 - acc: 0.8978 - val_loss: 8.4687 - val_acc: 0.1112\n",
            "Epoch 41/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.4830 - acc: 0.9038 - val_loss: 8.5306 - val_acc: 0.1144\n",
            "Epoch 42/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 0.4426 - acc: 0.9136 - val_loss: 8.5577 - val_acc: 0.1153\n",
            "Epoch 43/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 1.4714 - acc: 0.6877 - val_loss: 8.4013 - val_acc: 0.0949\n",
            "Epoch 44/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 1.4188 - acc: 0.6541 - val_loss: 8.4184 - val_acc: 0.1071\n",
            "Epoch 45/50\n",
            "6635/6635 [==============================] - 33s 5ms/step - loss: 0.6829 - acc: 0.8482 - val_loss: 8.4993 - val_acc: 0.1085\n",
            "Epoch 46/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 0.4940 - acc: 0.8995 - val_loss: 8.5614 - val_acc: 0.1112\n",
            "Epoch 47/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 0.4284 - acc: 0.9151 - val_loss: 8.6410 - val_acc: 0.1148\n",
            "Epoch 48/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 0.3780 - acc: 0.9234 - val_loss: 8.6580 - val_acc: 0.1153\n",
            "Epoch 49/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 0.3446 - acc: 0.9335 - val_loss: 8.7147 - val_acc: 0.1162\n",
            "Epoch 50/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 0.3272 - acc: 0.9356 - val_loss: 8.7619 - val_acc: 0.1184\n",
            "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('adadelta', 1)}\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_33 (Embedding)     (None, 54, 10)            21850     \n",
            "_________________________________________________________________\n",
            "lstm_46 (LSTM)               (None, 200)               168800    \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 2185)              439185    \n",
            "=================================================================\n",
            "Total params: 629,835\n",
            "Trainable params: 629,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6635 samples, validate on 2212 samples\n",
            "Epoch 1/50\n",
            "6635/6635 [==============================] - 39s 6ms/step - loss: 6.6196 - acc: 0.0514 - val_loss: 6.3347 - val_acc: 0.0484\n",
            "Epoch 2/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.2873 - acc: 0.0577 - val_loss: 6.3203 - val_acc: 0.0484\n",
            "Epoch 3/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.2634 - acc: 0.0579 - val_loss: 6.3201 - val_acc: 0.0484\n",
            "Epoch 4/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.2474 - acc: 0.0580 - val_loss: 6.3165 - val_acc: 0.0484\n",
            "Epoch 5/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.2382 - acc: 0.0580 - val_loss: 6.3089 - val_acc: 0.0484\n",
            "Epoch 6/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.2327 - acc: 0.0580 - val_loss: 6.3010 - val_acc: 0.0484\n",
            "Epoch 7/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.2210 - acc: 0.0580 - val_loss: 6.2974 - val_acc: 0.0484\n",
            "Epoch 8/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.2043 - acc: 0.0580 - val_loss: 6.2852 - val_acc: 0.0484\n",
            "Epoch 9/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.1944 - acc: 0.0580 - val_loss: 6.2739 - val_acc: 0.0484\n",
            "Epoch 10/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.1772 - acc: 0.0580 - val_loss: 6.2578 - val_acc: 0.0484\n",
            "Epoch 11/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.1468 - acc: 0.0603 - val_loss: 6.2362 - val_acc: 0.0497\n",
            "Epoch 12/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.1142 - acc: 0.0606 - val_loss: 6.2163 - val_acc: 0.0511\n",
            "Epoch 13/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.0767 - acc: 0.0619 - val_loss: 6.1929 - val_acc: 0.0533\n",
            "Epoch 14/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.0350 - acc: 0.0625 - val_loss: 6.1737 - val_acc: 0.0529\n",
            "Epoch 15/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.0016 - acc: 0.0630 - val_loss: 6.1586 - val_acc: 0.0561\n",
            "Epoch 16/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.9706 - acc: 0.0684 - val_loss: 6.1484 - val_acc: 0.0552\n",
            "Epoch 17/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.9314 - acc: 0.0707 - val_loss: 6.1312 - val_acc: 0.0592\n",
            "Epoch 18/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.9021 - acc: 0.0745 - val_loss: 6.1188 - val_acc: 0.0642\n",
            "Epoch 19/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.8752 - acc: 0.0751 - val_loss: 6.1099 - val_acc: 0.0669\n",
            "Epoch 20/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.8275 - acc: 0.0790 - val_loss: 6.1023 - val_acc: 0.0678\n",
            "Epoch 21/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.8023 - acc: 0.0829 - val_loss: 6.0936 - val_acc: 0.0732\n",
            "Epoch 22/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.7696 - acc: 0.0865 - val_loss: 6.0871 - val_acc: 0.0678\n",
            "Epoch 23/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.7264 - acc: 0.0846 - val_loss: 6.0749 - val_acc: 0.0705\n",
            "Epoch 24/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.6968 - acc: 0.0904 - val_loss: 6.0667 - val_acc: 0.0714\n",
            "Epoch 25/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.6482 - acc: 0.0977 - val_loss: 6.0669 - val_acc: 0.0737\n",
            "Epoch 26/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.6044 - acc: 0.1011 - val_loss: 6.0620 - val_acc: 0.0741\n",
            "Epoch 27/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.5669 - acc: 0.1026 - val_loss: 6.0584 - val_acc: 0.0759\n",
            "Epoch 28/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.5172 - acc: 0.1073 - val_loss: 6.0475 - val_acc: 0.0728\n",
            "Epoch 29/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.4832 - acc: 0.1096 - val_loss: 6.0520 - val_acc: 0.0800\n",
            "Epoch 30/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.4345 - acc: 0.1165 - val_loss: 6.0510 - val_acc: 0.0705\n",
            "Epoch 31/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.3816 - acc: 0.1201 - val_loss: 6.0438 - val_acc: 0.0701\n",
            "Epoch 32/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 5.3412 - acc: 0.1251 - val_loss: 6.0663 - val_acc: 0.0723\n",
            "Epoch 33/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.3002 - acc: 0.1251 - val_loss: 6.0478 - val_acc: 0.0732\n",
            "Epoch 34/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.2475 - acc: 0.1326 - val_loss: 6.0760 - val_acc: 0.0696\n",
            "Epoch 35/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.2087 - acc: 0.1402 - val_loss: 6.0575 - val_acc: 0.0732\n",
            "Epoch 36/50\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.1566 - acc: 0.1423 - val_loss: 6.0788 - val_acc: 0.0755\n",
            "Epoch 37/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.1030 - acc: 0.1489 - val_loss: 6.0893 - val_acc: 0.0692\n",
            "Epoch 38/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.0486 - acc: 0.1551 - val_loss: 6.0933 - val_acc: 0.0773\n",
            "Epoch 39/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 5.0078 - acc: 0.1598 - val_loss: 6.1027 - val_acc: 0.0778\n",
            "Epoch 40/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 4.9550 - acc: 0.1655 - val_loss: 6.1265 - val_acc: 0.0805\n",
            "Epoch 41/50\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 4.9025 - acc: 0.1747 - val_loss: 6.1397 - val_acc: 0.0805\n",
            "Epoch 42/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 4.8444 - acc: 0.1803 - val_loss: 6.1327 - val_acc: 0.0759\n",
            "Epoch 43/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.8048 - acc: 0.1784 - val_loss: 6.1606 - val_acc: 0.0759\n",
            "Epoch 44/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.7393 - acc: 0.1929 - val_loss: 6.1720 - val_acc: 0.0814\n",
            "Epoch 45/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 4.6869 - acc: 0.1910 - val_loss: 6.2154 - val_acc: 0.0805\n",
            "Epoch 46/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.6345 - acc: 0.2020 - val_loss: 6.2158 - val_acc: 0.0787\n",
            "Epoch 47/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.5939 - acc: 0.2078 - val_loss: 6.2182 - val_acc: 0.0782\n",
            "Epoch 48/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.5211 - acc: 0.2128 - val_loss: 6.2483 - val_acc: 0.0759\n",
            "Epoch 49/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.4761 - acc: 0.2199 - val_loss: 6.2795 - val_acc: 0.0800\n",
            "Epoch 50/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 4.4179 - acc: 0.2249 - val_loss: 6.2753 - val_acc: 0.0796\n",
            "Current Combination : {'activation': 'tanh', 'epochs': 50, 'layers': [('LSTM', 200), ('Dropout', 0.2)], 'optimizer': ('rmsprop', 0.1)}\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_34 (Embedding)     (None, 54, 10)            21850     \n",
            "_________________________________________________________________\n",
            "lstm_47 (LSTM)               (None, 200)               168800    \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 2185)              439185    \n",
            "=================================================================\n",
            "Total params: 629,835\n",
            "Trainable params: 629,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6635 samples, validate on 2212 samples\n",
            "Epoch 1/50\n",
            "6635/6635 [==============================] - 38s 6ms/step - loss: 15.8389 - acc: 0.0145 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 2/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 3/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 4/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 5/50\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 6/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 7/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 8/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 9/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 10/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 11/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 12/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 13/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 14/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 15/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 16/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 17/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 18/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 19/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 20/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 21/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 22/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 23/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 24/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 25/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 26/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 27/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 28/50\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 15.8800 - acc: 0.0148 - val_loss: 15.8849 - val_acc: 0.0145\n",
            "Epoch 29/50\n",
            "2460/6635 [==========>...................] - ETA: 16s - loss: 15.8757 - acc: 0.0150Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXTfa8M0X3V_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b79bd14-6ecf-47d0-ccb0-b286b0c2c2d0"
      },
      "source": [
        "m.best_accuracy"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.935644302632275"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh3f1aZe0YWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4d1b0e30-6272-40e3-d3d8-3f3ab0bdc2d2"
      },
      "source": [
        "m.best_parameters"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('LSTM', 200), ('Dropout', 0.2)],\n",
              " 'tanh',\n",
              " <keras.optimizers.Adam at 0x7f69dc151a20>,\n",
              " 0.001,\n",
              " 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la73qbwFICva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7f9e723-c6a6-403b-8ee9-01fc182042c7"
      },
      "source": [
        "print('Best Accuracy : {}, with best Parameters : {}'.format(m.best_accuracy*100, m.best_parameters))"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy : 93.56443026322751, with best Parameters : ([('LSTM', 200), ('Dropout', 0.2)], 'tanh', <keras.optimizers.Adam object at 0x7f69dc151a20>, 0.001, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A69KBMuuYCKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Sentences : \n",
        "def generate_n_sentences(n=5):\n",
        "    final_sentences = []\n",
        "    for _ in range(n):\n",
        "        prediction = x_test[np.random.randint(len(x_test))]\n",
        "        prediction = np.delete(prediction, 0)\n",
        "        first_prediction = m.best_model.predict_classes([x_test[0].reshape(1,54)])\n",
        "        prediction = np.append(prediction,first_prediction)\n",
        "        for _ in range(5):\n",
        "            next_prediction = m.best_model.predict_classes(prediction.reshape(1,54))\n",
        "            prediction = np.delete(prediction, 0)\n",
        "            prediction = np.append(prediction,next_prediction)\n",
        "\n",
        "\n",
        "\n",
        "        output_word = \"\"\n",
        "        for i in prediction:\n",
        "            if i:\n",
        "                for word,index in m.tokenizer.word_index.items():\n",
        "                    if index == i:\n",
        "                        output_word += word + ' '\n",
        "                        break\n",
        "\n",
        "        final_sentences.append(output_word)\n",
        "    return final_sentences"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6dDdSXlrxKc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "125e9b97-781d-4baf-81ab-6cd466e0b505"
      },
      "source": [
        "generate_n_sentences(10)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we negotiated a ceasefire in parts our terrible amp russia the russians ',\n",
              " 'should federal election commission and or fcc look into this there must be collusion with the democrats and of course russia such one sided media coverage most of it our fake news media collusion so ',\n",
              " 'slippery james comey the worst fbi director in history was not our scheme of the democrats lead ',\n",
              " '“i have seen all of the russian ads and i can say very definitively that swaying the election was not the main goal ”rob goldmanvice president of facebook our p more leakin’ i nice ',\n",
              " 'not associated our vindicates “trump” on russia and ',\n",
              " 'if it was the goal of russia to create discord our investigation to into the details ',\n",
              " 'why did the obama administration start an investigation into the trump campaign with zero proof of wrongdoing our country of this fbi against ',\n",
              " 'the mainstream media has refused to cover the fact that the head of the very important senate intelligence committee after two years of intensive study and access to our north korea north korea powerhouse ',\n",
              " 'remember it was our implanted a must gun tying ',\n",
              " '“no matter your ideologies or your loyalties this is a good day for america no our conspired to his have and ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },{
      "cell_type": "code",
      "metadata": {
        "id": "KQ2AjP6XIQMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
