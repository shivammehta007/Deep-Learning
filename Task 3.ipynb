{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OSp18hvBgSg",
        "colab_type": "text"
      },
      "source": [
        "* Read Language model tutorial ---> https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
        "* Find one english corpus with poetries in the internet (e.g from here) --> https://www.poetryfoundation.org/poems\n",
        "* You can use whatever corpus you want (e.g. your favorite book)\n",
        "* Encapsulate LSTM building like MLP from the first task\n",
        "* Train LSTM as language model on your corpus like in the tutorial\n",
        "* Also, you need to compare 1-layer and 2-layer LSTMs\n",
        "* Compare texts, generated by your models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3rdTupmBgSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import adam, adagrad, adadelta, rmsprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import L1L2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import ParameterGrid, train_test_split\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L38QIwIECiM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23aa40dc-6ab4-4805-ba62-357bbf619545"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KXJ2f9jBgSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read Data\n",
        "folder_name = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "filename = os.path.join(folder_name, 'task3_corpus.csv')\n",
        "# filename = 'task3_corpus.csv'\n",
        "file_type = 'csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fW7IwapBgS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(filename, file_type):\n",
        "    if file_type == 'csv':\n",
        "        data = pd.read_csv(filename)\n",
        "        data = data['text']\n",
        "        \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwGWpRi_BgTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = read_data(filename, file_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDLP6YnyBgUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = '\\n'.join([row for row in df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REwgMEzJBgUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "866bd057-7717-457c-c39d-29b583073e3a"
      },
      "source": [
        "layers = [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)]\n",
        "count = [x for x,_ in layers].count('LSTM')\n",
        "print(count)\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Z8RQoVBgUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelFormer:\n",
        "    def __init__(self):\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.best_model = Sequential()\n",
        "        self.best_accuracy = 0\n",
        "        self.best_parameters = {}\n",
        "        \n",
        "    def fit_data(self, text):\n",
        "        self.original_corpus = text\n",
        "        self.corpus = self.original_corpus.lower().split('\\n')\n",
        "        self.tokenizer.fit_on_texts(self.corpus)\n",
        "        self.word_count = len(self.tokenizer.word_index) + 1\n",
        "        input_sequences = []\n",
        "        for line in self.corpus:\n",
        "            tokens = self.tokenizer.texts_to_sequences([line])[0]\n",
        "            for i in range(1, len(tokens)):\n",
        "                n_grams_sequence = tokens[:i+1]\n",
        "                input_sequences.append(n_grams_sequence)\n",
        "        \n",
        "        input_sequences = self.pad_input_sequences(input_sequences)\n",
        "        \n",
        "        x_data, y_data = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "        y_data = np_utils.to_categorical(y_data, num_classes=self.word_count)\n",
        "        \n",
        "        return x_data, y_data\n",
        "              \n",
        "    def pad_input_sequences(self,input_sequences):\n",
        "        max_sequence_length = max([len(sentence) for sentence in input_sequences])\n",
        "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n",
        "        return input_sequences\n",
        "    \n",
        "    def fit(self, x_data, y_data , layers= [( 'LSTM', 150), ('Dropout', 0.2), ('LSTM', 120)], activation='tanh', optimizer='adam', lr=0.01, epochs=20):\n",
        "        self.model = Sequential()\n",
        "        \n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        x_train, x_val, y_train, y_val = train_test_split(self.x_data, self.y_data)\n",
        "        \n",
        "        \n",
        "        self.model.add(Embedding(self.word_count, 15, input_length=len(x_data[0]) ))\n",
        "        count_lstm_retn_flag = [x for x,_ in layers].count('LSTM') - 1\n",
        "\n",
        "        for layer,value in layers:\n",
        "            if layer == 'LSTM':\n",
        "                if count_lstm_retn_flag:\n",
        "                    count_lstm_retn_flag -= 1\n",
        "                    return_sequences = True \n",
        "                else:\n",
        "                    return_sequences = False\n",
        "                self.model.add(LSTM(value, activation=activation, return_sequences=return_sequences))\n",
        "            if layer == 'Dropout':\n",
        "                self.model.add(Dropout(value))\n",
        "        \n",
        "        self.model.add(Dense(self.word_count, activation='softmax'))\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = adam(lr=lr)\n",
        "        elif optimizer == 'adadelta':\n",
        "            optimizer = adadelta(lr=lr)\n",
        "        elif optimizer == 'rmsprop':\n",
        "            optimizer = rmsprop(lr=lr)\n",
        "            \n",
        "            \n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.model.summary()\n",
        "        \n",
        "        fit_summary = self.model.fit(x_train, y_train, epochs=epochs, verbose=1, validation_data=(x_val, y_val), batch_size=20)\n",
        "        if fit_summary.history['acc'][-1] > self.best_accuracy:\n",
        "            self.best_model = self.model\n",
        "            self.best_accuracy = fit_summary.history['acc'][-1]\n",
        "            self.best_parameters = (layers, activation, optimizer, lr, epochs)\n",
        "        \n",
        "        return fit_summary\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7C_bMBBgUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = ModelFormer()\n",
        "X, Y= m.fit_data(text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxxUqsczBgU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train , x_test, y_train, y_test = train_test_split(X,Y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEvPZQedBgVA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b8caedb-a5e9-449c-c08b-d53dcece7be2"
      },
      "source": [
        "print(len(x_train), len(y_train), len(x_test), len(y_test))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8847 8847 3792 3792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMKPolLxBgWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Grid Search with Parameter Grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOAkMxdABgWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters = { 'layers': [[( 'LSTM', 200), ('Dropout', 0.2), ('LSTM', 400), ('Dropout', 0.2) ], [( 'LSTM', 200), ('Dropout', 0.2)]], \n",
        "                     'activation': ['tanh'],\n",
        "                     'optimizer' : ['adam', 'adadelta'],\n",
        "                     'lr' : [0.01, 0.001],\n",
        "                     'epochs' : [20]\n",
        "                   }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unOtt-QJBgWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "778a87be-34d5-4786-ed41-4e597588deeb"
      },
      "source": [
        "combinations = list(ParameterGrid(hyperparameters))\n",
        "len(combinations)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmkHhKE0WiQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fit_summary_array = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EApzDLTlXP5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3264
        },
        "outputId": "441a29af-7984-4194-cb7e-7328c7276684"
      },
      "source": [
        "for combination in combinations:\n",
        "    print('Current Combination : {}'.format(combination))\n",
        "    m.fit(x_train, y_train, layers=combination['layers'], activation=combination['activation'], optimizer=combination['optimizer'], lr=combination['lr'], epochs=combination['epochs'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Combination : {'activation': 'tanh', 'epochs': 25, 'layers': [('LSTM', 10), ('Dropout', 0.1)], 'lr': 0.1, 'optimizer': 'adam'}\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 54, 15)            32775     \n",
            "_________________________________________________________________\n",
            "lstm_29 (LSTM)               (None, 10)                1040      \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 2185)              24035     \n",
            "=================================================================\n",
            "Total params: 57,850\n",
            "Trainable params: 57,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6635 samples, validate on 2212 samples\n",
            "Epoch 1/25\n",
            "6635/6635 [==============================] - 36s 5ms/step - loss: 6.8253 - acc: 0.0493 - val_loss: 6.6685 - val_acc: 0.0570\n",
            "Epoch 2/25\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.2608 - acc: 0.0576 - val_loss: 6.7903 - val_acc: 0.0570\n",
            "Epoch 3/25\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.1996 - acc: 0.0577 - val_loss: 6.8618 - val_acc: 0.0669\n",
            "Epoch 4/25\n",
            "6635/6635 [==============================] - 30s 4ms/step - loss: 6.1754 - acc: 0.0613 - val_loss: 6.9128 - val_acc: 0.0696\n",
            "Epoch 5/25\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.1548 - acc: 0.0635 - val_loss: 6.9856 - val_acc: 0.0710\n",
            "Epoch 6/25\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.1508 - acc: 0.0601 - val_loss: 7.0286 - val_acc: 0.0692\n",
            "Epoch 7/25\n",
            "6635/6635 [==============================] - 29s 4ms/step - loss: 6.2038 - acc: 0.0597 - val_loss: 7.1191 - val_acc: 0.0678\n",
            "Epoch 8/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.2141 - acc: 0.0591 - val_loss: 7.1410 - val_acc: 0.0696\n",
            "Epoch 9/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.1396 - acc: 0.0610 - val_loss: 7.1528 - val_acc: 0.0470\n",
            "Epoch 10/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.1301 - acc: 0.0630 - val_loss: 7.1529 - val_acc: 0.0669\n",
            "Epoch 11/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.1263 - acc: 0.0641 - val_loss: 7.1650 - val_acc: 0.0719\n",
            "Epoch 12/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.0740 - acc: 0.0660 - val_loss: 7.1329 - val_acc: 0.0818\n",
            "Epoch 13/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.0413 - acc: 0.0692 - val_loss: 7.1728 - val_acc: 0.0687\n",
            "Epoch 14/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.0744 - acc: 0.0665 - val_loss: 7.2062 - val_acc: 0.0732\n",
            "Epoch 15/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.0547 - acc: 0.0660 - val_loss: 7.1935 - val_acc: 0.0660\n",
            "Epoch 16/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.0482 - acc: 0.0628 - val_loss: 7.2123 - val_acc: 0.0683\n",
            "Epoch 17/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.0271 - acc: 0.0633 - val_loss: 7.2152 - val_acc: 0.0606\n",
            "Epoch 18/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.9952 - acc: 0.0610 - val_loss: 7.1658 - val_acc: 0.0741\n",
            "Epoch 19/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.0054 - acc: 0.0693 - val_loss: 7.1900 - val_acc: 0.0778\n",
            "Epoch 20/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.9700 - acc: 0.0705 - val_loss: 7.1766 - val_acc: 0.0714\n",
            "Epoch 21/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.9876 - acc: 0.0687 - val_loss: 7.1956 - val_acc: 0.0601\n",
            "Epoch 22/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.9614 - acc: 0.0714 - val_loss: 7.1961 - val_acc: 0.0778\n",
            "Epoch 23/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 5.9253 - acc: 0.0707 - val_loss: 7.1603 - val_acc: 0.0814\n",
            "Epoch 24/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.9035 - acc: 0.0696 - val_loss: 7.1825 - val_acc: 0.0791\n",
            "Epoch 25/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 5.9145 - acc: 0.0660 - val_loss: 7.1749 - val_acc: 0.0796\n",
            "Current Combination : {'activation': 'tanh', 'epochs': 25, 'layers': [('LSTM', 10), ('Dropout', 0.1)], 'lr': 0.1, 'optimizer': 'adadelta'}\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_20 (Embedding)     (None, 54, 15)            32775     \n",
            "_________________________________________________________________\n",
            "lstm_30 (LSTM)               (None, 10)                1040      \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 2185)              24035     \n",
            "=================================================================\n",
            "Total params: 57,850\n",
            "Trainable params: 57,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6635 samples, validate on 2212 samples\n",
            "Epoch 1/25\n",
            "6635/6635 [==============================] - 37s 6ms/step - loss: 7.6839 - acc: 0.0488 - val_loss: 7.6780 - val_acc: 0.0642\n",
            "Epoch 2/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.6714 - acc: 0.0582 - val_loss: 7.6660 - val_acc: 0.0642\n",
            "Epoch 3/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.6567 - acc: 0.0582 - val_loss: 7.6494 - val_acc: 0.0642\n",
            "Epoch 4/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.6301 - acc: 0.0582 - val_loss: 7.6054 - val_acc: 0.0642\n",
            "Epoch 5/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.5593 - acc: 0.0582 - val_loss: 7.5074 - val_acc: 0.0642\n",
            "Epoch 6/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.4355 - acc: 0.0582 - val_loss: 7.3471 - val_acc: 0.0642\n",
            "Epoch 7/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.2506 - acc: 0.0582 - val_loss: 7.1690 - val_acc: 0.0642\n",
            "Epoch 8/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.1082 - acc: 0.0582 - val_loss: 7.0661 - val_acc: 0.0642\n",
            "Epoch 9/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.0115 - acc: 0.0582 - val_loss: 6.9781 - val_acc: 0.0642\n",
            "Epoch 10/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.9194 - acc: 0.0582 - val_loss: 6.8987 - val_acc: 0.0642\n",
            "Epoch 11/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.8386 - acc: 0.0582 - val_loss: 6.8273 - val_acc: 0.0642\n",
            "Epoch 12/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.7676 - acc: 0.0582 - val_loss: 6.7609 - val_acc: 0.0642\n",
            "Epoch 13/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.7015 - acc: 0.0582 - val_loss: 6.7016 - val_acc: 0.0642\n",
            "Epoch 14/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.6403 - acc: 0.0582 - val_loss: 6.6486 - val_acc: 0.0642\n",
            "Epoch 15/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.5926 - acc: 0.0582 - val_loss: 6.6018 - val_acc: 0.0642\n",
            "Epoch 16/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.5423 - acc: 0.0582 - val_loss: 6.5599 - val_acc: 0.0642\n",
            "Epoch 17/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.5008 - acc: 0.0582 - val_loss: 6.5224 - val_acc: 0.0642\n",
            "Epoch 18/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.4627 - acc: 0.0582 - val_loss: 6.4894 - val_acc: 0.0642\n",
            "Epoch 19/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.4261 - acc: 0.0582 - val_loss: 6.4599 - val_acc: 0.0642\n",
            "Epoch 20/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.3997 - acc: 0.0582 - val_loss: 6.4342 - val_acc: 0.0642\n",
            "Epoch 21/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.3743 - acc: 0.0582 - val_loss: 6.4116 - val_acc: 0.0642\n",
            "Epoch 22/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.3494 - acc: 0.0582 - val_loss: 6.3920 - val_acc: 0.0642\n",
            "Epoch 23/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.3257 - acc: 0.0582 - val_loss: 6.3744 - val_acc: 0.0642\n",
            "Epoch 24/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 6.3090 - acc: 0.0582 - val_loss: 6.3597 - val_acc: 0.0642\n",
            "Epoch 25/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 6.2971 - acc: 0.0582 - val_loss: 6.3460 - val_acc: 0.0642\n",
            "Current Combination : {'activation': 'tanh', 'epochs': 25, 'layers': [('LSTM', 10), ('Dropout', 0.1)], 'lr': 0.1, 'optimizer': 'rmsprop'}\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_21 (Embedding)     (None, 54, 15)            32775     \n",
            "_________________________________________________________________\n",
            "lstm_31 (LSTM)               (None, 10)                1040      \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 2185)              24035     \n",
            "=================================================================\n",
            "Total params: 57,850\n",
            "Trainable params: 57,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6635 samples, validate on 2212 samples\n",
            "Epoch 1/25\n",
            "6635/6635 [==============================] - 37s 6ms/step - loss: 7.2237 - acc: 0.0567 - val_loss: 7.6343 - val_acc: 0.0710\n",
            "Epoch 2/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.6012 - acc: 0.0628 - val_loss: 7.9178 - val_acc: 0.0705\n",
            "Epoch 3/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.6968 - acc: 0.0650 - val_loss: 7.9617 - val_acc: 0.0696\n",
            "Epoch 4/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.7687 - acc: 0.0681 - val_loss: 8.0254 - val_acc: 0.0764\n",
            "Epoch 5/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.7631 - acc: 0.0701 - val_loss: 8.0116 - val_acc: 0.0764\n",
            "Epoch 6/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.8140 - acc: 0.0719 - val_loss: 8.0854 - val_acc: 0.0759\n",
            "Epoch 7/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.8653 - acc: 0.0722 - val_loss: 8.1229 - val_acc: 0.0696\n",
            "Epoch 8/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.8882 - acc: 0.0739 - val_loss: 8.0899 - val_acc: 0.0805\n",
            "Epoch 9/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.8784 - acc: 0.0743 - val_loss: 8.1230 - val_acc: 0.0746\n",
            "Epoch 10/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.8598 - acc: 0.0716 - val_loss: 8.1092 - val_acc: 0.0710\n",
            "Epoch 11/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.8694 - acc: 0.0725 - val_loss: 8.0976 - val_acc: 0.0759\n",
            "Epoch 12/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.8636 - acc: 0.0754 - val_loss: 8.0949 - val_acc: 0.0764\n",
            "Epoch 13/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.8759 - acc: 0.0767 - val_loss: 8.1667 - val_acc: 0.0714\n",
            "Epoch 14/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.9024 - acc: 0.0755 - val_loss: 8.1622 - val_acc: 0.0719\n",
            "Epoch 15/25\n",
            "6635/6635 [==============================] - 31s 5ms/step - loss: 7.9096 - acc: 0.0773 - val_loss: 8.1777 - val_acc: 0.0719\n",
            "Epoch 16/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 7.9361 - acc: 0.0779 - val_loss: 8.2160 - val_acc: 0.0723\n",
            "Epoch 17/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 8.0000 - acc: 0.0782 - val_loss: 8.3307 - val_acc: 0.0737\n",
            "Epoch 18/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 8.0365 - acc: 0.0755 - val_loss: 8.2084 - val_acc: 0.0755\n",
            "Epoch 19/25\n",
            "6635/6635 [==============================] - 30s 5ms/step - loss: 8.0255 - acc: 0.0782 - val_loss: 8.1849 - val_acc: 0.0759\n",
            "Epoch 20/25\n",
            "5360/6635 [=======================>......] - ETA: 5s - loss: 8.0225 - acc: 0.0793"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXTfa8M0X3V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}